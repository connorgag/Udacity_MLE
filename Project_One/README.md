# Optimizing an ML Pipeline in Azure
By Connor Gag

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
This dataset contains data about individuals that a bank marketed towards; we seek to predict whether or not they will create a term deposit with the bank. 

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The best performing model was a Voting Ensemble created by AutoML. 
However, while the accuracy for AutoML was higher than the Hyperdrive model, it was not very significant (< 1% difference). 


## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
This pipeline takes in the data, cleans it using train.py, then uses Hyperdrive to tune the parameters. The given hyperparameters to tune are batch size, learning rate, and keep probability. The classification algorithm that it uses is Logistic Regression.

It uses Random Parameter Sampling and the Bandit Early Stopping Policy. 

**What are the benefits of the parameter sampler you chose?**
The benefit of using random parameter sampling is that it is significantly faster than grid sampling, yet it doesn't significantly decrease the accuracy. This works well for projects like this, when it isn't necessary to exhaust unnecessary resources. 

**What are the benefits of the early stopping policy you chose?**
The benefit of using the Bandit policy is that it focuses on creating a single model that maximizes the primary metric. Bandit stops any jobs that are a certain amount below the threshold set by the current best model. This saves time by preventing Hyperdrive from spending time and resources on jobs that haven't shown initial results. 


## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
AutoML used Voting Ensemble, which combines the result of multiple algorithms to classify the target; here are a few of the algorithms used within it: LightGBM, XGBoostClassifier, and SGD. 
For LightGBM, some of the hyperparameters that it tuned were learning rate (~ 0.0578) and num_leaves (137). 


## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
Hyperdrive had an accuracy of 0.91139. 
AutoML had an accuracy of 0.91800. 

I expected AutoML to be significantly better because it tests multiple kinds of algorithms, but this was not the case. VotingEnsemble using a balanced approach by combining the output of multiple models, so this could account for the slightly higher accuracy. Hyperdrive only used Logistic Regression and didn't try out any other models, so AutoML has the upperhand when it comes to testing out a variety of multiple algorithms as well as hyperparameters. AutoML was much quicker to create, while resulting in the same, if not better, accuracy than Hyperdrive. 

In any case, the difference in accuracy is negligable, but the architecture for AutoML is significantly simpler. 

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
One improvement would be to add more hyperparameters to the hyperdrive Logistic Regression. For example, we could add solver and/or penalty to our parameter sampler. 

Another improvement is to try out other classification algorithms other than Logistic Regression. This would help level the playing field with AutoML, which tries out multiple algorithms. 

Lastly, we could also test out using different primary metrics. For example, when I ran: 

`get_primary_metrics('classification')`

The result was 
`['average_precision_score_weighted',
 'accuracy',
 'precision_score_weighted',
 'norm_macro_recall',
 'AUC_weighted']`

 Any of these other options would work. 